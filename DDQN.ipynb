{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.7.6)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import get_env\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "# from priority_experience_replay import Memory\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = get_env.get_env(observation_type = \"buckets\", reward_grid_size = (20,20)) # trying a smaller grid\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DDQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DDQN, self).__init__()\n",
        "        self.inputlayer= torch.nn.Linear(18, 32, bias=True)\n",
        "        self.hl1= torch.nn.Linear(32, 16, bias=True)\n",
        "        self.hl2= torch.nn.Linear(16, 8, bias=True)\n",
        "        self.value= torch.nn.Linear(8, 1, bias=True)\n",
        "        self.adv= torch.nn.Linear(8, 9, bias=True)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(x.size(0), -1) # flatten but conserve batches\n",
        "        x = F.relu(self.inputlayer(x))\n",
        "        x = F.relu(self.hl1(x))\n",
        "        x = F.relu(self.hl2(x))\n",
        "        v = self.value(x)\n",
        "        a = self.adv(x)\n",
        "        q = v + a - a.mean(dim=1, keepdim=True)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections import deque\n",
        "\n",
        "class Memory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def append_experience(experience):\n",
        "    memory.add(experience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.5\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 5\n",
        "TARGET_UPDATE = 2\n",
        "NUM_SKIP = 40\n",
        "ALLOW_0 = 50 # allow action 0 (no movement) after 50 episodes\n",
        "\n",
        "policy_net = DDQN().to(device)\n",
        "target_net = DDQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.001)\n",
        "memory = Memory(20000)\n",
        "\n",
        "\n",
        "def select_action(state, episode):\n",
        "    # if greedy action is 0 for episodes before allow 0, take a random action\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * episode / EPS_DECAY)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            a =  policy_net(state).max(1)[1].view(1, 1)\n",
        "        if a == 0 and episode < ALLOW_0:\n",
        "            return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)\n",
        "        else:\n",
        "            return a\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    batch = memory.sample(BATCH_SIZE)\n",
        "    batch = [*zip(*batch)]\n",
        "\n",
        "    state_batch = torch.cat(batch[0])\n",
        "    action_batch = torch.cat(batch[1])\n",
        "    reward_batch = torch.cat(batch[2])\n",
        "    next_states_batch = torch.cat(batch[3])\n",
        "    dones_batch = torch.cat(batch[4])\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # Compute the expected Q values\n",
        "    next_state_values = target_net(next_states_batch).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) * (1 - dones_batch) + reward_batch\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def motivation(state, action):\n",
        "    # buckets state assumed - size 8,2\n",
        "    global steps_done\n",
        "    x = state[:,0] * torch.exp(-0.1 * state[:,1])\n",
        "    motivation = 0.001 * (action != 0) * torch.max(x)\n",
        "    return motivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_state(observation, info):\n",
        "    observation[:,0]/=100\n",
        "    observation[:,1]/=1000\n",
        "    rel_cordinates = np.array(info['relative_coordinates'])/10000\n",
        "    x = np.concatenate([observation.flatten(), rel_cordinates])\n",
        "    return torch.tensor([x], device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('targetnet_ddqn'): os.makedirs('targetnet_ddqn') # save folder for value networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. ALLOW_0: take random action if the greedy action is 0 (no movement) for all episodes < ALLOW_0\n",
        "2. Kill agent if it fails to collect berry within the first minute\n",
        "3. kill agent if cumilative reward reaches 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.verbose = False\n",
        "episode_durations = []\n",
        "episode_rewards = []\n",
        "episode_berrypicked = []\n",
        "num_episodes = 250\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # Initialize the environment and state\n",
        "    observation, done = env.reset()\n",
        "    info = env.get_info()\n",
        "\n",
        "    state = make_state(observation, info)\n",
        "    num_steps = 0\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        # Select and perform an action\n",
        "        action = select_action(state, episode=10000)\n",
        "        for _ in range(NUM_SKIP-1):\n",
        "            next_observation, env_reward, done, next_info = env.step(action.item())\n",
        "            num_steps += 1\n",
        "\n",
        "            reward = torch.tensor([env_reward], device=device, dtype=torch.float32)\n",
        "            done = torch.tensor([done], device=device, dtype=torch.int)\n",
        "\n",
        "            # Observe new state\n",
        "            next_state = make_state(next_observation, next_info)\n",
        "\n",
        "            # Store the transition in memory\n",
        "            append_experience((state, action, reward, next_state, done))\n",
        "\n",
        "            if done: break\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "                \n",
        "        env.render()\n",
        "        if env.cummulative_reward < 0:\n",
        "            print(\"episode: \", i_episode, \"agent died of hunger. Picked:\", env.get_numBerriesPicked())\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break            \n",
        "\n",
        "        if done:\n",
        "            print(\"episode: \", i_episode, \" reward: \", env.cummulative_reward, \"Picked:\", env.get_numBerriesPicked())\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "\n",
        "        if num_steps > 400*60 and env.get_numBerriesPicked() == 0:\n",
        "            print(\"episode: \", i_episode, \"agent killed for being berryless for too long.\")\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "    \n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        torch.save(target_net.state_dict(), f'targetnet_ddqn/targetnet_ep{i_episode}.pth')\n",
        "\n",
        "print('Complete')\n",
        "# env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
