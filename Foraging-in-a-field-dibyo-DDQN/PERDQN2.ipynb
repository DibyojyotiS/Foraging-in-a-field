{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import get_env\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "# from priority_experience_replay import Memory\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = get_env.get_env(observation_type = \"buckets\", reward_grid_size = (20,20))  # trying a smaller grid\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.inputlayer= torch.nn.Linear(18, 32, bias=True)\n",
        "        self.hl1= torch.nn.Linear(32, 8, bias=True)\n",
        "        self.hl2= torch.nn.Linear(8, 8, bias=True)\n",
        "        self.outlayer= torch.nn.Linear(8, 9, bias=True)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(x.size(0), -1) # flatten but conserve batches\n",
        "        x = F.relu(self.inputlayer(x))\n",
        "        x = F.relu(self.hl1(x))\n",
        "        x = F.relu(self.hl2(x))\n",
        "        x = self.outlayer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    e = 0.01\n",
        "    a = 0.6\n",
        "    beta = 0.4\n",
        "    beta_increment_per_sampling = 0.001\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        return (np.abs(error) + self.e) ** self.a\n",
        "\n",
        "    def add(self, error, sample):\n",
        "        assert len(sample) == 5\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)\n",
        "\n",
        "    def sample(self, n):\n",
        "        batch = []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / n\n",
        "        priorities = np.zeros(n)\n",
        "\n",
        "        self.beta = min(1., self.beta + self.beta_increment_per_sampling)\n",
        "\n",
        "        i = 0\n",
        "        while i < n:\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "\n",
        "            s = random.uniform(a, b)\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            if data is None: continue\n",
        "            priorities[i] = p\n",
        "            batch.append(data)\n",
        "            idxs.append(idx)\n",
        "            i+=1\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total() + 10E-8 # for zero priority events\n",
        "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        return batch, idxs, is_weight\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tree.n_entries\n",
        "\n",
        "\n",
        "\n",
        "# SumTree\n",
        "# a binary tree data structure where the parentâ€™s value is the sum of its children\n",
        "class SumTree:\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = [None for _ in range(capacity)]\n",
        "        self.n_entries = 0\n",
        "\n",
        "    # update to the root node\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    # find sample on leaf node\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    # store priority and sample\n",
        "    def add(self, p, data):\n",
        "        assert len(data) == 5\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "    # update priority\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    # get priority and sample\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_experience(experience):\n",
        "    state, action, reward, next_state, done = experience\n",
        "    target = reward\n",
        "    if done == 0: target = target + target_net(next_state).max(1)[0].detach()\n",
        "    error = (target - policy_net(state).gather(1, action)).detach()\n",
        "    memory.add(error=error.squeeze().cpu(), sample=experience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 50\n",
        "TARGET_UPDATE = 2\n",
        "\n",
        "ALLOW_0 = 50 # allow action 0 (no movement) after 50 episodes\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.001)\n",
        "memory = Memory(20000)\n",
        "\n",
        "\n",
        "def select_action(state, episode):\n",
        "    # if greedy action is 0 for episodes before allow 0, take a random action\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * episode / EPS_DECAY)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            a =  policy_net(state).max(1)[1].view(1, 1)\n",
        "        if a == 0 and episode < ALLOW_0:\n",
        "            return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)\n",
        "        else:\n",
        "            return a\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    batch, idxs, is_weights = memory.sample(BATCH_SIZE)\n",
        "    is_weights = torch.tensor(is_weights, device=device, dtype=torch.float32).to(device)\n",
        "    batch = [*zip(*batch)]\n",
        "\n",
        "    state_batch = torch.cat(batch[0])\n",
        "    action_batch = torch.cat(batch[1])\n",
        "    reward_batch = torch.cat(batch[2])\n",
        "    next_states_batch = torch.cat(batch[3])\n",
        "    dones_batch = torch.cat(batch[4])\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # Compute the expected Q values\n",
        "    next_state_values = target_net(next_states_batch).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) * (1 - dones_batch) + reward_batch\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    # criterion = nn.SmoothL1Loss()\n",
        "    # loss = criterion(state_action_values, expected_state_action_values)\n",
        "    loss = (is_weights * F.smooth_l1_loss(state_action_values, expected_state_action_values, reduction='none')).mean()\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    # update priorities\n",
        "    error = torch.abs(expected_state_action_values - state_action_values).detach().squeeze().cpu()\n",
        "    for i in range(BATCH_SIZE):\n",
        "        memory.update(idxs[i], error[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def motivation(state, action):\n",
        "    # buckets state assumed - size 8,2\n",
        "    global steps_done\n",
        "    x = state[:,0] * torch.exp(-0.1 * state[:,1])\n",
        "    motivation = 0.001 * (action != 0) * torch.max(x)\n",
        "    return motivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_state(observation, info):\n",
        "    observation[:,0]/=100\n",
        "    observation[:,1]/=1000\n",
        "    rel_cordinates = np.array(info['relative_coordinates'])/10000\n",
        "    x = np.concatenate([observation.flatten(), rel_cordinates])\n",
        "    return torch.tensor([x], device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('targetnet_perdqn2'): os.makedirs('targetnet_perdqn2') # save folder for value networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. ALLOW_0: take random action if the greedy action is 0 (no movement) for all episodes < ALLOW_0\n",
        "2. Kill agent if it fails to collect berry within the first minute\n",
        "3. kill agent if cumilative reward falls to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load appropriate file if  required\n",
        "policy_net.load_state_dict('targetnet_perdqn2/targetnet_ep52.pth')\n",
        "target_net.load_state_dict(policy_net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.verbose = True\n",
        "# env.MAX_STEPS = 400*60\n",
        "\n",
        "episode_durations = []\n",
        "episode_rewards = []\n",
        "episode_berrypicked = []\n",
        "num_episodes = 250\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # Initialize the environment and state\n",
        "    observation, done = env.reset()\n",
        "    info = env.get_info()\n",
        "\n",
        "    state = make_state(observation, info)\n",
        "    for t in count():\n",
        "        \n",
        "        # Select and perform an action\n",
        "        action = select_action(state, episode=i_episode)\n",
        "        next_observation, env_reward, done, next_info = env.step(action.item())\n",
        "\n",
        "        reward = torch.tensor([env_reward], device=device, dtype=torch.float32)\n",
        "        done = torch.tensor([done], device=device, dtype=torch.int)\n",
        "\n",
        "        # Observe new state\n",
        "        next_state = make_state(next_observation, next_info)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        append_experience((state, action, reward, next_state, done))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "                \n",
        "        env.render()\n",
        "        if env.cummulative_reward < 0:\n",
        "            print(\"episode: \", i_episode, \"agent died of hunger.\")\n",
        "            episode_durations.append(t + 1)\n",
        "            episode_rewards.append(env_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break            \n",
        "\n",
        "        if done:\n",
        "            print(\"episode: \", i_episode, \" reward: \", env.cummulative_reward)\n",
        "            episode_durations.append(t + 1)\n",
        "            episode_rewards.append(env_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "\n",
        "        if t > 400*60 and env.get_numBerriesPicked() == 0:\n",
        "            print(\"episode: \", i_episode, \"agent killed for being berryless for too long.\")\n",
        "            episode_durations.append(t + 1)\n",
        "            episode_rewards.append(env_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "    \n",
        "    for i in range(100):\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        torch.save(target_net.state_dict(), f'targetnet_perdqn2/targetnet_ep{i_episode}.pth')\n",
        "\n",
        "print('Complete')\n",
        "# env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
