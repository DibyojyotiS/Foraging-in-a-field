{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.0.1 (SDL 2.0.14, Python 3.9.6)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import get_env\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "# from priority_experience_replay import Memory\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = get_env.get_env(observation_type = \"buckets\", reward_grid_size = (1000,1000), reward_curiosity_beta=0.1) # trying a different grid sizes for curiosity rewards\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DDQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DDQN, self).__init__()\n",
        "        self.inputlayer= torch.nn.Linear(18, 32, bias=True)\n",
        "        self.hl1= torch.nn.Linear(32, 16, bias=True)\n",
        "        self.hl2= torch.nn.Linear(16, 8, bias=True)\n",
        "        self.value= torch.nn.Linear(8, 1, bias=True)\n",
        "        self.adv= torch.nn.Linear(8, 9, bias=True)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(x.size(0), -1) # flatten but conserve batches\n",
        "        x = F.relu(self.inputlayer(x))\n",
        "        x = F.relu(self.hl1(x))\n",
        "        x = F.relu(self.hl2(x))\n",
        "        v = self.value(x)\n",
        "        a = self.adv(x)\n",
        "        q = v + a - a.mean(dim=1, keepdim=True)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections import deque\n",
        "\n",
        "class Memory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "def append_experience(experience):\n",
        "    memory.add(experience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "\n",
        "EPS_START = 0.8\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 20          # for epsilon decay\n",
        "TARGET_UPDATE = 2       \n",
        "NUM_SKIP = 40           # number of steps to skip\n",
        "KILL_BERRYLESS = 400*60 # kill agent if it had collected no berries in these many stepss\n",
        "\n",
        "\n",
        "policy_net = DDQN().to(device)\n",
        "target_net = DDQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.001)\n",
        "memory = Memory(20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load(i):\n",
        "    policy_net.load_state_dict(torch.load(f'targetnet_ddqn/targetnet_ep{i}.pth'))\n",
        "    target_net.load_state_dict(torch.load(f'targetnet_ddqn/targetnet_ep{i}.pth'))\n",
        "\n",
        "load(146)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_state(observation, info):\n",
        "    observation[:,0]/=100\n",
        "    observation[:,1]/=1000\n",
        "    rel_cordinates = np.array(info['relative_coordinates'])/10000\n",
        "    x = np.concatenate([observation.flatten(), rel_cordinates])\n",
        "    return torch.tensor([x], device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_action(state, episode, enable_0=False):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * episode / EPS_DECAY)\n",
        "    sample = 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            a =  policy_net(state).max(1)[1].view(1, 1)\n",
        "        if a == 0 and not enable_0:\n",
        "            return torch.tensor([[random.randrange(1, 9)]], device=device, dtype=torch.long)\n",
        "        else:\n",
        "            return a\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(1-enable_0, 9)]], device=device, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def motivation(observation, action):\n",
        "    # buckets state assumed - size 8,2\n",
        "    global steps_done\n",
        "    x = observation[:,0] * torch.exp(-0.1 * observation[:,1])\n",
        "    motivation = 0.001 * (action != 0) * torch.max(x)\n",
        "    return motivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "heuristic_distDiscount_rate = 0.01\n",
        "heuristic_last_action = 0\n",
        "def heuristic_action(observation):\n",
        "    discounted_sizes = observation[:,0]*np.exp(-heuristic_distDiscount_rate * observation[:,1])\n",
        "    action = np.argmax(discounted_sizes) + 1\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    batch = memory.sample(BATCH_SIZE)\n",
        "    batch = [*zip(*batch)]\n",
        "\n",
        "    state_batch = torch.cat(batch[0])\n",
        "    action_batch = torch.cat(batch[1])\n",
        "    reward_batch = torch.cat(batch[2])\n",
        "    next_states_batch = torch.cat(batch[3])\n",
        "    dones_batch = torch.cat(batch[4])\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # Compute the expected Q values\n",
        "    next_state_values = target_net(next_states_batch).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) * (1 - dones_batch) + reward_batch\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values)\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('targetnet_ddqn'): os.makedirs('targetnet_ddqn') # save folder for value networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. ALLOW_0: take random action if the greedy action is 0 (no movement) for all episodes < ALLOW_0\n",
        "2. Kill agent if it fails to collect berry within the first minute\n",
        "3. kill agent if cumilative reward reaches 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "episode_durations = []\n",
        "episode_rewards = []\n",
        "episode_berrypicked = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "observation, done = env.reset()\n",
        "info = env.get_info()\n",
        "\n",
        "state = make_state(observation, info)\n",
        "num_steps = 0\n",
        "\n",
        "while True:\n",
        "        \n",
        "        # Select and perform an action\n",
        "        action = select_action(state, episode=200, enable_0 = env.cummulative_reward > 0.6)\n",
        "        for _ in range(NUM_SKIP-1):\n",
        "            next_observation, env_reward, done, next_info = env.step(action.item())\n",
        "            num_steps += 1\n",
        "\n",
        "            reward = torch.tensor([env_reward], device=device, dtype=torch.float32)\n",
        "            done = torch.tensor([done], device=device, dtype=torch.int)\n",
        "\n",
        "            # Observe new state\n",
        "            next_state = make_state(next_observation, next_info)\n",
        "\n",
        "            # Store the transition in memory\n",
        "\n",
        "            if done: break\n",
        "            \n",
        "\n",
        "        #env.render()\n",
        "        # Move to the next state\n",
        "        env.render()\n",
        "        state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode:  0 agent killed for being berryless for too long.\n",
            "episode:  1 agent died of hunger. Picked: 1\n",
            "episode:  2 agent died of hunger. Picked: 2\n",
            "episode:  3 agent died of hunger. Picked: 3\n",
            "episode:  4 agent died of hunger. Picked: 1\n",
            "episode:  5 agent died of hunger. Picked: 1\n",
            "episode:  6 agent died of hunger. Picked: 4\n",
            "episode:  7 agent died of hunger. Picked: 5\n",
            "episode:  8 agent died of hunger. Picked: 1\n",
            "episode:  9 agent killed for being berryless for too long.\n",
            "episode:  10 agent died of hunger. Picked: 1\n",
            "episode:  11 agent died of hunger. Picked: 4\n",
            "episode:  12 agent died of hunger. Picked: 1\n",
            "episode:  13 agent killed for being berryless for too long.\n",
            "episode:  14 agent died of hunger. Picked: 4\n",
            "episode:  15 agent killed for being berryless for too long.\n",
            "episode:  16 agent died of hunger. Picked: 3\n",
            "episode:  17 agent killed for being berryless for too long.\n",
            "episode:  18 agent died of hunger. Picked: 4\n",
            "episode:  19 agent killed for being berryless for too long.\n",
            "episode:  20 agent died of hunger. Picked: 2\n",
            "episode:  21 agent died of hunger. Picked: 5\n",
            "episode:  22 agent died of hunger. Picked: 1\n",
            "episode:  23 agent died of hunger. Picked: 11\n",
            "episode:  24 agent died of hunger. Picked: 2\n",
            "episode:  25 agent died of hunger. Picked: 2\n",
            "episode:  26 agent died of hunger. Picked: 2\n",
            "episode:  27 agent died of hunger. Picked: 5\n",
            "episode:  28 agent killed for being berryless for too long.\n",
            "episode:  29 agent died of hunger. Picked: 2\n",
            "episode:  30 agent died of hunger. Picked: 3\n",
            "episode:  31 agent died of hunger. Picked: 2\n",
            "episode:  32 agent died of hunger. Picked: 7\n",
            "episode:  33 agent killed for being berryless for too long.\n",
            "episode:  34 agent died of hunger. Picked: 5\n",
            "episode:  35 agent killed for being berryless for too long.\n",
            "episode:  36 agent killed for being berryless for too long.\n",
            "episode:  37 agent died of hunger. Picked: 5\n",
            "episode:  38 agent died of hunger. Picked: 1\n",
            "episode:  39 agent died of hunger. Picked: 6\n",
            "episode:  40 agent killed for being berryless for too long.\n",
            "episode:  41 agent died of hunger. Picked: 1\n",
            "episode:  42 agent died of hunger. Picked: 9\n",
            "episode:  43 agent died of hunger. Picked: 3\n",
            "episode:  44 agent died of hunger. Picked: 1\n",
            "episode:  45 agent died of hunger. Picked: 1\n",
            "episode:  46 agent died of hunger. Picked: 2\n",
            "episode:  47 agent died of hunger. Picked: 2\n",
            "episode:  48 agent killed for being berryless for too long.\n",
            "episode:  49 agent died of hunger. Picked: 5\n",
            "episode:  50 agent died of hunger. Picked: 3\n",
            "episode:  51 agent died of hunger. Picked: 3\n",
            "episode:  52 agent died of hunger. Picked: 2\n",
            "episode:  53 agent died of hunger. Picked: 3\n",
            "episode:  54 agent died of hunger. Picked: 10\n",
            "episode:  55 agent died of hunger. Picked: 4\n",
            "episode:  56 agent killed for being berryless for too long.\n",
            "episode:  57 agent died of hunger. Picked: 5\n",
            "episode:  58 agent died of hunger. Picked: 11\n",
            "episode:  59 agent killed for being berryless for too long.\n",
            "episode:  60 agent killed for being berryless for too long.\n",
            "episode:  61 agent killed for being berryless for too long.\n",
            "episode:  62 agent died of hunger. Picked: 3\n",
            "episode:  63 agent died of hunger. Picked: 2\n",
            "episode:  64 agent died of hunger. Picked: 2\n",
            "episode:  65 agent died of hunger. Picked: 2\n",
            "episode:  66 agent killed for being berryless for too long.\n",
            "episode:  67 agent died of hunger. Picked: 8\n",
            "episode:  68 agent killed for being berryless for too long.\n",
            "episode:  69 agent died of hunger. Picked: 2\n",
            "episode:  70 agent died of hunger. Picked: 5\n",
            "episode:  71 agent died of hunger. Picked: 1\n",
            "episode:  72 agent died of hunger. Picked: 4\n",
            "episode:  73 agent died of hunger. Picked: 5\n",
            "episode:  74 agent killed for being berryless for too long.\n",
            "episode:  75 agent killed for being berryless for too long.\n",
            "episode:  76 agent died of hunger. Picked: 1\n",
            "episode:  77 agent died of hunger. Picked: 2\n",
            "episode:  78 agent died of hunger. Picked: 4\n",
            "episode:  79 agent died of hunger. Picked: 1\n",
            "episode:  80 agent died of hunger. Picked: 7\n",
            "episode:  81 agent died of hunger. Picked: 4\n",
            "episode:  82 agent died of hunger. Picked: 3\n",
            "episode:  83 agent died of hunger. Picked: 4\n",
            "episode:  84 agent died of hunger. Picked: 2\n",
            "episode:  85 agent died of hunger. Picked: 3\n",
            "episode:  86 agent died of hunger. Picked: 2\n",
            "episode:  87 agent died of hunger. Picked: 3\n",
            "episode:  88 agent died of hunger. Picked: 2\n",
            "episode:  89 agent killed for being berryless for too long.\n",
            "episode:  90 agent died of hunger. Picked: 1\n",
            "episode:  91 agent killed for being berryless for too long.\n",
            "episode:  92 agent killed for being berryless for too long.\n",
            "episode:  93 agent killed for being berryless for too long.\n",
            "episode:  94 agent died of hunger. Picked: 2\n",
            "episode:  95 agent killed for being berryless for too long.\n",
            "episode:  96 agent killed for being berryless for too long.\n",
            "episode:  97 agent died of hunger. Picked: 1\n",
            "episode:  98 agent died of hunger. Picked: 5\n",
            "episode:  99 agent died of hunger. Picked: 4\n",
            "episode:  100 agent died of hunger. Picked: 2\n",
            "episode:  101 agent died of hunger. Picked: 8\n",
            "episode:  102 agent died of hunger. Picked: 4\n",
            "episode:  103 agent died of hunger. Picked: 3\n",
            "episode:  104 agent died of hunger. Picked: 3\n",
            "episode:  105 agent killed for being berryless for too long.\n",
            "episode:  106 agent died of hunger. Picked: 3\n",
            "episode:  107 agent died of hunger. Picked: 1\n",
            "episode:  108 agent died of hunger. Picked: 1\n",
            "episode:  109 agent died of hunger. Picked: 3\n",
            "episode:  110 agent died of hunger. Picked: 8\n",
            "episode:  111 agent died of hunger. Picked: 7\n",
            "episode:  112 agent died of hunger. Picked: 2\n",
            "episode:  113 agent died of hunger. Picked: 6\n",
            "episode:  114 agent died of hunger. Picked: 2\n",
            "episode:  115 agent killed for being berryless for too long.\n",
            "episode:  116 agent died of hunger. Picked: 6\n",
            "episode:  117 agent killed for being berryless for too long.\n",
            "episode:  118 agent died of hunger. Picked: 4\n",
            "episode:  119 agent died of hunger. Picked: 1\n",
            "episode:  120 agent died of hunger. Picked: 2\n",
            "episode:  121 agent died of hunger. Picked: 4\n",
            "episode:  122 agent died of hunger. Picked: 4\n",
            "episode:  123 agent died of hunger. Picked: 3\n",
            "episode:  124 agent died of hunger. Picked: 3\n",
            "episode:  125 agent died of hunger. Picked: 6\n",
            "episode:  126 agent died of hunger. Picked: 3\n",
            "episode:  127 agent died of hunger. Picked: 2\n",
            "episode:  128 agent died of hunger. Picked: 4\n",
            "episode:  129 agent died of hunger. Picked: 4\n",
            "episode:  130 agent killed for being berryless for too long.\n",
            "episode:  131 agent died of hunger. Picked: 1\n",
            "episode:  132 agent killed for being berryless for too long.\n",
            "episode:  133 agent died of hunger. Picked: 4\n",
            "episode:  134 agent died of hunger. Picked: 1\n",
            "episode:  135 agent died of hunger. Picked: 3\n",
            "episode:  136 agent died of hunger. Picked: 5\n",
            "episode:  137 agent died of hunger. Picked: 5\n",
            "episode:  138 agent died of hunger. Picked: 4\n",
            "episode:  139 agent died of hunger. Picked: 2\n",
            "episode:  140 agent died of hunger. Picked: 5\n",
            "episode:  141 agent killed for being berryless for too long.\n",
            "episode:  142 agent died of hunger. Picked: 2\n",
            "episode:  143 agent died of hunger. Picked: 7\n",
            "episode:  144 agent died of hunger. Picked: 8\n",
            "episode:  145 agent died of hunger. Picked: 2\n",
            "episode:  146 agent died of hunger. Picked: 3\n",
            "episode:  147 agent killed for being berryless for too long.\n",
            "episode:  148 agent killed for being berryless for too long.\n",
            "episode:  149 agent died of hunger. Picked: 2\n",
            "episode:  150 agent died of hunger. Picked: 2\n",
            "episode:  151 agent died of hunger. Picked: 3\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (36,5) into shape (35,5)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15516/2658272221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcummulative_reward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_SKIP\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mnext_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\varma\\documents\\repositories\\ddqn_foraging\\foraging-in-a-field\\env\\berry-field\\berry_field\\envs\\berry_field_mat_input_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjuice_reward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mliving_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcummulative_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\varma\\documents\\repositories\\ddqn_foraging\\foraging-in-a-field\\env\\berry-field\\berry_field\\envs\\berry_field_mat_input_env.py\u001b[0m in \u001b[0;36mget_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munordered_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbucket_obseration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\varma\\documents\\repositories\\ddqn_foraging\\foraging-in-a-field\\env\\berry-field\\berry_field\\envs\\berry_field_mat_input_env.py\u001b[0m in \u001b[0;36mbucket_obseration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m             2nd column: average distance to berries in a bucket \"\"\"\n\u001b[0;32m    215\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUMBUCKETS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munordered_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[0mberries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\varma\\documents\\repositories\\ddqn_foraging\\foraging-in-a-field\\env\\berry-field\\berry_field\\envs\\berry_field_mat_input_env.py\u001b[0m in \u001b[0;36munordered_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mdirections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirections\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (36,5) into shape (35,5)"
          ]
        }
      ],
      "source": [
        "env.verbose = False\n",
        "num_episodes = 250\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # Initialize the environment and state\n",
        "    observation, done = env.reset()\n",
        "    info = env.get_info()\n",
        "\n",
        "    state = make_state(observation, info)\n",
        "    num_steps = 0\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        # Select and perform an action\n",
        "        action = select_action(state, episode=i_episode, enable_0 = env.cummulative_reward > 0.6)\n",
        "        for _ in range(NUM_SKIP-1):\n",
        "            next_observation, env_reward, done, next_info = env.step(action.item())\n",
        "            num_steps += 1\n",
        "\n",
        "            reward = torch.tensor([env_reward], device=device, dtype=torch.float32)\n",
        "            done = torch.tensor([done], device=device, dtype=torch.int)\n",
        "\n",
        "            # Observe new state\n",
        "            next_state = make_state(next_observation, next_info)\n",
        "\n",
        "            # Store the transition in memory\n",
        "            append_experience((state, action, reward, next_state, done))\n",
        "\n",
        "            if done: break\n",
        "            if num_steps > KILL_BERRYLESS and env.get_numBerriesPicked() == 0: break\n",
        "\n",
        "        #env.render()\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "                \n",
        "        if env.cummulative_reward < 0:\n",
        "            print(\"episode: \", i_episode, \"agent died of hunger. Picked:\", env.get_numBerriesPicked())\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break            \n",
        "\n",
        "        if done:\n",
        "            print(\"episode: \", i_episode, \" reward: \", env.cummulative_reward, \"Picked:\", env.get_numBerriesPicked())\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "\n",
        "        if num_steps > KILL_BERRYLESS and env.get_numBerriesPicked() == 0:\n",
        "            print(\"episode: \", i_episode, \"agent killed for being berryless for too long.\")\n",
        "            episode_durations.append(num_steps)\n",
        "            episode_rewards.append(env.cummulative_reward)\n",
        "            episode_berrypicked.append(env.get_numBerriesPicked())\n",
        "            break\n",
        "    \n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        torch.save(target_net.state_dict(), f'targetnet_ddqn/targetnet_ep{i_episode}.pth')\n",
        "\n",
        "print('Complete')\n",
        "# env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "data = {\n",
        "    'episode_durations': episode_durations,\n",
        "    'episode_rewards': episode_rewards,\n",
        "    'episode_berrypicked': episode_berrypicked\n",
        "}\n",
        "with open('targetnet_ddqn/data.txt', 'w') as outfile:\n",
        "    json.dump(data, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
