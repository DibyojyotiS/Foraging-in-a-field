{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import get_env\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "# from priority_experience_replay import Memory\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = get_env.get_env(observation_type = \"buckets\")\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.inputlayer= torch.nn.Linear(18, 32, bias=True)\n",
        "        self.hl1= torch.nn.Linear(32, 8, bias=True)\n",
        "        self.hl2= torch.nn.Linear(8, 8, bias=True)\n",
        "        self.outlayer= torch.nn.Linear(8, 9, bias=True)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x.view(x.size(0), -1) # flatten but conserve batches\n",
        "        x = F.relu(self.inputlayer(x))\n",
        "        x = F.relu(self.hl1(x))\n",
        "        x = F.relu(self.hl2(x))\n",
        "        x = self.outlayer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    e = 0.01\n",
        "    a = 0.6\n",
        "    beta = 0.4\n",
        "    beta_increment_per_sampling = 0.001\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        return (np.abs(error) + self.e) ** self.a\n",
        "\n",
        "    def add(self, error, sample):\n",
        "        assert len(sample) == 5\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)\n",
        "\n",
        "    def sample(self, n):\n",
        "        batch = []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / n\n",
        "        priorities = np.zeros(n)\n",
        "\n",
        "        self.beta = min(1., self.beta + self.beta_increment_per_sampling)\n",
        "\n",
        "        i = 0\n",
        "        while i < n:\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "\n",
        "            s = random.uniform(a, b)\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            if data is None: continue\n",
        "            priorities[i] = p\n",
        "            batch.append(data)\n",
        "            idxs.append(idx)\n",
        "            i+=1\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total() + 10E-8 # for zero priority events\n",
        "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        return batch, idxs, is_weight\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tree.n_entries\n",
        "\n",
        "\n",
        "\n",
        "# SumTree\n",
        "# a binary tree data structure where the parentâ€™s value is the sum of its children\n",
        "class SumTree:\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = [None for _ in range(capacity)]\n",
        "        self.n_entries = 0\n",
        "\n",
        "    # update to the root node\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    # find sample on leaf node\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    # store priority and sample\n",
        "    def add(self, p, data):\n",
        "        assert len(data) == 5\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "    # update priority\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    # get priority and sample\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 50\n",
        "TARGET_UPDATE = 2\n",
        "\n",
        "policy_net = DQN().to(device)\n",
        "target_net = DQN().to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=0.001)\n",
        "memory = Memory(20000)\n",
        "\n",
        "\n",
        "def select_action(state, episode):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * episode / EPS_DECAY)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "def append_experience(experience):\n",
        "    state, action, reward, next_state, done = experience\n",
        "    target = reward\n",
        "    if done == 0: target = target + target_net(next_state).max(1)[0].detach()\n",
        "    error = (target - policy_net(state).gather(1, action)).detach()\n",
        "    memory.add(error=error.squeeze().cpu(), sample=experience)\n",
        "\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "episode_rewards = []\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    # rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    batch, idxs, is_weights = memory.sample(BATCH_SIZE)\n",
        "    is_weights = torch.tensor(is_weights, device=device, dtype=torch.float32).to(device)\n",
        "    batch = [*zip(*batch)]\n",
        "\n",
        "    state_batch = torch.cat(batch[0])\n",
        "    action_batch = torch.cat(batch[1])\n",
        "    reward_batch = torch.cat(batch[2])\n",
        "    next_states_batch = torch.cat(batch[3])\n",
        "    dones_batch = torch.cat(batch[4])\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # Compute the expected Q values\n",
        "    next_state_values = target_net(next_states_batch).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) * (1 - dones_batch) + reward_batch\n",
        "    expected_state_action_values = expected_state_action_values.unsqueeze(1)\n",
        "\n",
        "    # Compute Huber loss\n",
        "    # criterion = nn.SmoothL1Loss()\n",
        "    # loss = criterion(state_action_values, expected_state_action_values)\n",
        "    loss = (is_weights * F.smooth_l1_loss(state_action_values, expected_state_action_values, reduction='none')).mean()\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    # update priorities\n",
        "    error = torch.abs(expected_state_action_values - state_action_values).detach().squeeze().cpu()\n",
        "    for i in range(BATCH_SIZE):\n",
        "        memory.update(idxs[i], error[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def motivation(state, action):\n",
        "    # buckets state assumed - size 8,2\n",
        "    global steps_done\n",
        "    x = state[:,0] * torch.exp(-0.1 * state[:,1])\n",
        "    motivation = 0.001 * (action != 0) * torch.max(x)\n",
        "    return motivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists('targetnet_perdqn2'): os.makedirs('targetnet_perdqn2') # save folder for value networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_state(observation, info):\n",
        "    observation[:,0]/=100\n",
        "    observation[:,1]/=1000\n",
        "    rel_cordinates = np.array(info['relative_coordinates'])/10000\n",
        "    x = np.concatenate([observation.flatten(), rel_cordinates])\n",
        "    return torch.tensor([x], device=device, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "picked  1 berries\n",
            "episode:  0  reward:  -0.620770833334379\n",
            "episode:  1  reward:  -0.6290625000010509\n",
            "episode:  2  reward:  -0.4615520833341796\n",
            "episode:  3  reward:  -0.6310312500010533\n",
            "episode:  4  reward:  -0.6329375000010556\n",
            "episode:  5  reward:  -0.6282395833343832\n",
            "episode:  6  reward:  -0.39370833333409666\n"
          ]
        }
      ],
      "source": [
        "env.verbose = True\n",
        "# env.MAX_STEPS = 400*60\n",
        "num_episodes = 250\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # Initialize the environment and state\n",
        "    observation, done = env.reset()\n",
        "    info = env.get_info()\n",
        "\n",
        "    state = make_state(observation, info)\n",
        "    for t in count():\n",
        "        \n",
        "        # Select and perform an action\n",
        "        action = select_action(state, episode=i_episode)\n",
        "        next_observation, env_reward, done, next_info = env.step(action.item())\n",
        "\n",
        "        # motivation_reward = motivation(state, action)\n",
        "        # reward = torch.tensor([env_reward+motivation_reward], device=device, dtype=torch.float32)\n",
        "        reward = torch.tensor([env_reward], device=device, dtype=torch.float32)\n",
        "        done = torch.tensor([done], device=device, dtype=torch.int)\n",
        "\n",
        "        # Observe new state\n",
        "        next_state = make_state(next_observation, next_info)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        append_experience((state, action, reward, next_state, done))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "                \n",
        "        # env.render()\n",
        "        if done:\n",
        "            print(\"episode: \", i_episode, \" reward: \", env.cummulative_reward)\n",
        "            episode_durations.append(t + 1)\n",
        "            episode_rewards.append(env_reward)\n",
        "            break\n",
        "    \n",
        "    for i in range(100):\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        torch.save(target_net.state_dict(), f'targetnet_perdqn2/targetnet_ep{i_episode}.pth')\n",
        "\n",
        "print('Complete')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
